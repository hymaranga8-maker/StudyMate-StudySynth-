# -*- coding: utf-8 -*-
"""StudyMate_program.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/172UNAZZPHUz_xubkAtNAabtBwehmhdW6
"""

# Study Mate - Compact Version
!pip install -q gradio transformers torch sentence-transformers PyPDF2 faiss-cpu deep-translator accelerate

import gradio as gr
import PyPDF2
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import faiss
from deep_translator import GoogleTranslator

print("Loading models...")
tokenizer = AutoTokenizer.from_pretrained("ibm-granite/granite-3.0-2b-instruct")
model = AutoModelForCausalLM.from_pretrained(
    "ibm-granite/granite-3.0-2b-instruct",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)
embedder = SentenceTransformer('all-MiniLM-L6-v2')
print("âœ… Models loaded!")

# Globals
chunks, index = [], None
LANGS = {"English":"en","Hindi":"hi","Bengali":"bn","Telugu":"te","Marathi":"mr",
         "Tamil":"ta","Gujarati":"gu","Kannada":"kn","Malayalam":"ml","Punjabi":"pa"}

def extract_pdf(file):
    path = file.name if hasattr(file, 'name') else file
    text = ""
    with open(path, "rb") as f:
        for page in PyPDF2.PdfReader(f).pages:
            text += page.extract_text() + "\n"
    return text if text.strip() else "No text found"

def process_pdf(file):
    global chunks, index
    if not file: return "Upload PDF first"
    text = extract_pdf(file)
    if "No text" in text: return text

    words = text.split()
    chunks = [" ".join(words[i:i+500]) for i in range(0, len(words), 450)]
    embs = embedder.encode(chunks, convert_to_numpy=True)
    index = faiss.IndexFlatL2(embs.shape[1])
    index.add(embs)
    return f"âœ… Processed {len(chunks)} chunks"

def get_context(query):
    if not index or not chunks: return ""
    q_emb = embedder.encode([query], convert_to_numpy=True)
    _, ids = index.search(q_emb, 3)
    return "\n\n".join([chunks[i] for i in ids[0] if i < len(chunks)])

def translate(text, lang):
    if lang == "English" or not text: return text
    try:
        return GoogleTranslator(source='auto', target=LANGS[lang]).translate(text[:4500])
    except: return text

def chat(question, lang, history):
    if not question.strip(): return history, history

    ctx = get_context(question)
    prompt = f"Context:\n{ctx}\n\nQuestion: {question}\n\nProvide a concise answer." if ctx else f"Question: {question}\n\nProvide a brief answer."

    inputs = tokenizer.apply_chat_template(
        [{"role": "user", "content": prompt}],
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(model.device)

    with torch.no_grad():
        out = model.generate(
            inputs,
            max_new_tokens=256,  # Reduced from 512
            temperature=0.7,
            do_sample=False,  # Greedy decoding for speed
            pad_token_id=tokenizer.eos_token_id,
            num_beams=1  # No beam search
        )

    response = tokenizer.decode(out[0][inputs.shape[1]:], skip_special_tokens=True)
    response = translate(response, lang)
    history.append((question, response))
    return history, history

# UI
with gr.Blocks(theme=gr.themes.Soft()) as app:
    gr.Markdown("# ðŸ“š Study Mate")
    chat_state = gr.State([])

    with gr.Row():
        with gr.Column(scale=1):
            pdf = gr.File(label="ðŸ“„ Upload PDF", file_types=[".pdf"])
            status = gr.Textbox(label="Status", interactive=False)
            gr.Button("Process PDF").click(process_pdf, pdf, status)
            lang = gr.Dropdown(list(LANGS.keys()), value="English", label="ðŸŒ Language")
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(height=400)
            question = gr.Textbox(label="ðŸ’¬ Ask Question", placeholder="Type here...")
            with gr.Row():
                send_btn = gr.Button("Send")
                clear_btn = gr.Button("Clear")

    send_btn.click(chat, [question, lang, chat_state], [chatbot, chat_state]).then(
        lambda: "", None, question)
    question.submit(chat, [question, lang, chat_state], [chatbot, chat_state]).then(
        lambda: "", None, question)
    clear_btn.click(lambda: ([], []), None, [chatbot, chat_state])

app.launch(share=True)